% The following homework template was modified from the Fall 2017 CIS 160 homework template.

\documentclass{article}
\usepackage[letterpaper,top=0.5in,bottom=0.5in,left=1.2in,right=1.2in,includeheadfoot]{geometry}
\usepackage{amssymb,amsmath}
\usepackage{parskip}
\usepackage{fancyhdr}
\usepackage{tabu}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{amsthm}
\usepackage{listings}
\usepackage{amsmath}

\renewcommand{\baselinestretch}{1.3}
\pagestyle{fancy}
\fancyfoot{}
\renewcommand{\headrulewidth}{0pt}
\setlength{\headheight}{0pt}

\fancypagestyle{firstpage}{
  \fancyhead{}
  \fancyfoot{}
}

% Modify here to change homework number, name, and PennKey as necessary.
\newcommand{\HomeworkNo}{3}
\newcommand{\MyName}{Kevin Monogue (kmonogue@stanford.edu)}

\fancyhead[L]{\MyName}

\fancyhead[R]{Winter 2020 Midterm Exam}

\newcommand{\PrintFirstHeader}{
  CME 241 \vspace{5pt} \hfill {\Large{\MyName}}
  \\
  {\LARGE{\textbf{Linear Systems Bellman Error Minimization}}} 

  \rule{\textwidth}{0.4pt}}
  
\begin{document}
\thispagestyle{firstpage}
\PrintFirstHeader{}

% Write homework solutions here.
The motivation behind this derivation is to investigate how we might best be able to solve an RL problem using linear value function approximation. In a general setting, we know we have converged at the true value function when we reach the stationary point of the Bellman operator. At this point, updating our value function with new experiences provides no change to the value function. However, with function approximation, we are of course "approximating" the true value function, and thus are unlikely to arrive at the perfectly defined corollary to the tabular setting. Instead, we'd like to investigate different interpretations of defining/minimizing the Bellman Error. \\

We first try to minimize the distance between the value function and the Bellman updated value function.
\begin{align*}
w_{BE} & = argmin_w d(v_w, R_\pi + \gamma P_\pi v_w) 
\end{align*}
We can rewrite our value function in terms of our linear approximation. 
\begin{align*}
& = argmin_w d(\phi \cdot w, R_\pi + \gamma P_\pi \cdot \phi \cdot w) 
\end{align*}
Since the RHS is a sum of vectors, we can subtract one component to both elements of the distance function in order to rearrange terms and then re-factor
\begin{align*}
& = argmin_w d(\phi \cdot w - \gamma P_\pi \cdot \phi \cdot w, R_\pi ) \\
& = argmin_w d((\phi - \gamma P_\pi \cdot \phi) \cdot w, R_\pi ) 
\end{align*}
Notice that $R_\pi$ is a vector of scalars, as are $\phi$, $P_\pi$ (matrix), and $\gamma$ (single value). The only variable we are minimizing over is $w$. Thus, this is equivalent to fitting a set of features $(\phi - \gamma P_\pi \cdot \phi) = X$ via a parameter $w$ to fit a target $R_\pi = Y$. This can be solved with the common least squares formula. Recall that our definition of distance is weighted by $\mu_\pi$ for each state, requiring us to weight the regression by diagonal matrix of weights $D$.
\begin{align*}
w_{BE} & = (X^TX)^{-1} X^TY \\
& =  ((\phi - \gamma P_\pi \cdot \phi)^T D (\phi - \gamma P_\pi \cdot \phi))^{-1} \cdot (\phi - \gamma P_\pi \cdot \phi)^T \cdot D \cdot  R_\pi
\end{align*}
\\
Another approach is to minimize the distance between the value function approximation and the projected Bellman operator applied to the value function. Visually, the projected Bellman operator applies the Bellman operator to the value function, then projects it back into the subspace spanned by the approximations feature space. This differs from before by this notion of projection - it keeps us in the space of the function approximation while still applying the Bellman update. First we define the projection operator
\begin{align*}
\Pi_\phi = \phi \cdot (\phi^T \cdot D \cdot \phi)^{-1} \cdot \phi^T \cdot D
\end{align*}
This is simply the projection onto the weighted subspace of $\phi$ (derived from the least squares solution minimizing the distance from a point on subspace to v). Recall we want to find the point where applying the projected Bellman update yields no difference in our value function (stationary point).
\begin{align*}
\phi w_{PBE} & = \Pi_\phi \cdot B\pi \cdot \phi \cdot w_{PBE} \\
\phi w_{PBE} & = \phi \cdot (\phi^T \cdot D \cdot \phi)^{-1} \cdot \phi^T \cdot D \cdot (R_\pi + \gamma P_\pi \cdot \phi \cdot w_{PBE}) \\
w_{PBE} & = (\phi^T \cdot D \cdot \phi)^{-1} \cdot \phi^T \cdot D \cdot (R_\pi + \gamma P_\pi \cdot \phi \cdot w_{PBE}) \\
 (\phi^T \cdot D \cdot \phi) \cdot w_{PBE} & = \phi^T \cdot D \cdot (R_\pi + \gamma P_\pi \cdot \phi \cdot w_{PBE}) \\
  (\phi^T \cdot D \cdot \phi) \cdot w_{PBE} - \phi^T \cdot D \cdot \gamma P_\pi \cdot \phi \cdot w_{PBE} & = \phi^T \cdot D \cdot R_\pi \\
  \phi^T \cdot D \cdot (\phi - \gamma P_\pi \cdot \phi) \cdot w_{PBE} & = \phi^T \cdot D \cdot R_\pi
\end{align*}
First note that we are able to invert $\phi$ because we assume it's columns to be linearly independent. Notice the last line from above is simply a linear system. $\phi^T \cdot D \cdot (\phi - \gamma P_\pi \cdot \phi)$ is a scalar matrix, $\phi^T \cdot D \cdot R_\pi$ is a scalar vector, and we are trying to solve for $w$. Thus, 
\begin{align*}
w_{PBE} = (\phi^T \cdot D \cdot (\phi - \gamma P_\pi \cdot \phi)^{-1} \cdot \phi^T \cdot D \cdot R_\pi
\end{align*}
This provides a direct solution to minimizing the projected Bellman error for a linear function approximation.
\end{document}
