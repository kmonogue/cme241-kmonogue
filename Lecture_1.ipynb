{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1 - MP/MRP/MDP Overview\n",
    "### Notes and Code for Basic Data Structures\n",
    "Source for notes: [CME 241 Lecture Slides](http://web.stanford.edu/class/cme241/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Property\n",
    "\n",
    "All relevant information for future actions/states/outcomes is dependent only on the current action/state/outcome. That is, the string of all historical steps in a process does not matter at any point - only the relevant information for the observed period matters. Formally, this means that any type of expectation for the future need only be conditioned on the current state, rather than a history of any number of states.\n",
    "\n",
    "$$ P[x_{t+1} | x_{t}] = P[x_{t+1} | x_{t}, x_{t-1}, ..., x_{1}] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the processes we are discussing, $x$ from above can be replaced with $s$ to represent a state. **State transition probability** is thus the probability of being at a new state $s'$ given the current state $s$.\n",
    "$$P[s' | s] = P_{ss'}$$\n",
    "read as \"probability from s to s'\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $n$ states, the probability of obtaining any state given the current state can be summarized as a matrix. We have $n$ rows pertaining to each possible option of the current state, and $n$ columns pertaining to the next state. For example, entry $P_{2, 3}$ represents the probability we move to state 3 given we are at state 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Process\n",
    "\n",
    "A Markov Process is simply a series of states and transitions defined by a state transition probability matrix $P$. It must maintain the Markov property. An example possible series states (i.e. transitions with non-zero probabilities), is called an episode and finishes upon arrival to a terminal state (zero probability to transition anywhere $P_{Ts} = 0$ for all $s$).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Reward Process\n",
    "\n",
    "Same as above, except now there are rewards associated with each state. Existing or arriving in any state is associated with some reward value. Typically, this framework is used to represent the relative value of a given episode of states and transitions. $R_s$ is usually used to define the expected reward given the current state $s$. That is:\n",
    "\n",
    "$$R_s = E[R_{t+1} | S_t = s]$$\n",
    "\n",
    "Read this as the immediate reward associated with state $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also useful to have a sense of discounting future rewards when we consider multi-period situations. This is analogous to the time-value discounting of money. This is useful in MPs to account for uncertainty in the future, to avoid cycles in certain processes (as there is now decay), and to accurately represent the present value of certain types of real world rewards (such as money). A factor $\\gamma \\in [0, 1]$ is used to discount future rewards. It's application can be seen clearly in calculating total return over a time period:\n",
    "$$G_t = R_{t+1} + \\gamma \\cdot R_{t+2} + ... = \\sum_{k=0}^{\\inf} \\gamma^{k} \\cdot R_{t + k + 1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read this as the total reward over all future time periods from now, point $t$. Future rewards are discounted by the discount factor $\\gamma$ described above. This function describes a particular episode or set of future positions. It is useful to instead use this function to describe the value of a state $s$. Given a state $s$, we would like to know the expected return of existing at this state. Thus, we can take the expectation across all return values associated with this $s$ to describe the value of the state. The value function is:\n",
    "$$v(s) = E[G_t | S_t = s]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing G from above, it is clear this function depends on the value of $\\gamma$. For example, if $\\gamma = 0$, then all future rewards will be discounted to nothing. Thus, the value function for any given state is exactly equal to its immediate reward. On the other hand, $\\gamma = 1$ values future rewards as highly as current rewards and the value function may be more complex to calculate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon further evaluation of the value function, it can be decomposed into two separate parts. The immediate reward, and all future rewards. But, notice that the future rewards can be expressed as the discounted value function of the next state $S_{t+1}$. Mathematically:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v(s) & = E[G_t | S_t = s] \\\\\n",
    "& = E[R_{t+1} + \\gamma \\cdot R_{t+2} + ... | S_t = s] \\\\\n",
    "& = E[R_{t+1} + \\gamma \\cdot G_{t+1} | S_t = s] \\\\\n",
    "& = E[R_{t+1} + \\gamma \\cdot v(S_{t+1}) | S_t = s] \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we can use the state transition probability matrix to compute the expectation of the value function across possible future states. Notice the first part of the equation $E[R_{t+1} | S_t = s]$ is simply our earlier description of the immediate reward. We can now rewrite our value function as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v(s) & = E[R_{t+1} + \\gamma \\cdot v(S_{t+1}) | S_t = s] \\\\\n",
    "& = E[R_{t+1} | S_t = s] + E[\\gamma \\cdot v(S_{t+1}) | S_t = s] \\\\\n",
    "& = R_s + \\gamma \\sum_{s' \\in S} P_{ss'} \\cdot v(s')\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This is the ever important **Bellman Equation** for MRPs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line decomposes the value function into the two components. First, there is the immediate reward for the state we are evaluating at $s$. Next, we discount the value obtained at the next state, and compute this value by summing across all possible next states in accordance with their probability of occurring. Note the recursive nature of this formula. The formula can also be expressed recursively as \n",
    "\n",
    "$$v = R + \\gamma P v$$\n",
    "\n",
    "where v and R are column vectors with one entry per state. The state probability matrix P multiplies each row by the column vector v to sum across possible next states value functions. This makes the Bellman Equation a linear problem, which can be solved directly. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v & = R + \\gamma P v \\\\\n",
    "v - \\gamma P v & = R \\\\ \n",
    "(I - \\gamma P) v & = R \\\\\n",
    "v & = (I - \\gamma P)^{-1} + R\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this has complexity $O(n^3)$ where n is the number of states. Iterative methods may be used instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDP is very similar to MRP, however it adds the concept of **actions**. Actions are elements that take us from one state to another. Incorporating the concept of action in our model allows us to better describe how we move, or may move, from one state to another. Naturally, our representations of state transitions should now incorporate actions. \n",
    "\n",
    "$$ P_{ss'}^{a} = E[S_{t+1} = s' | A_t = a, S_t = s] $$\n",
    "$$ R_s^a = E[R_{t+1} | A_t = a, S_t = s] $$\n",
    "\n",
    "The key here is that the probability we reach state $s'$ is not only dependent on the state we are in, but also the action that we take from that state. Additionally, the immediate reward is dependent on the action we take. We should then have a representation to describe what actions are possible, or how actions are likely to be taken, given a state, in order for this definition to also make sense. A policy $\\pi$ is used to describe the probability distribution of actions given a state.\n",
    "\n",
    "$$\\pi(a|s) = P[A_t = a | S_t = s] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is Markov (only depends on the current state) and also stationary (does not depend on the value of $t$). Policies define the behavior of an agent in a MP. That is, given a MP, the existence of a policy defines how an agent will move from state to state and thus operate in the MP. Note how different policies will have different value functions, as the likelihood of being in different states changes with a change in policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice also how we can move backwards from this level of abstraction by defining our earlier components of a MP or a MRP for a given/specific policy. That is, once we choose a policy $\\pi$, all of our earlier components are defined as they were before. Take for example just a sequence of states and probabilities, i.e. a MP. This is defined by $S$ and $P^\\pi$ once $\\pi$ is defined. Similarly, for a Markov reward process, we can reproduce our earlier P and R by defining a policy and then summing across actions. Similarly, the value function is simply defined by policy now as well:\n",
    "\n",
    "$$ P_{ss'}^\\pi = \\sum_{a \\in A} \\pi(a|s) \\cdot P_{ss'}^a $$\n",
    "$$ R_s^\\pi = \\sum_{a \\in A} \\pi(a|s) \\cdot R_{s}^a $$\n",
    "$$ v_\\pi (s) = E[G_t | S_t = s] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define the value function to not only account for the current state but also the current action for a bit more granularity. This is called the action-value function and is useful for certain computational features of an MRP. It is defined as:\n",
    "\n",
    "$$q_\\pi (a, s) = E[G_t | S_t = s, A_t = a] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the value function can be obtained by summing across all actions in the action-value function and incorporating the probability of taking each action\n",
    "\n",
    "$$v_\\pi(s) = \\sum_{a \\in A} q_\\pi(a, s) \\cdot \\pi(a|s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Bellman Expectation Equation** is the MDP version of the Bellman equation and can be written by summing across the possible actions from a state $s$:\n",
    "\n",
    "$$v_\\pi(s) = \\sum_{a \\in A} \\pi(a | s) \\cdot [R_s^a + \\gamma \\sum_{s' \\in S} P_{ss'}^a \\cdot v_\\pi(s')]$$\n",
    "\n",
    "A corrolary can also be written for the action-value function:\n",
    "\n",
    "$$ q_\\pi(a, s) = R_s^a + \\gamma \\sum_{s' \\in S} P_{ss'}^a \\cdot v_\\pi(s') $$\n",
    "\n",
    "Notice that the Bellman expectation equation above for the value function uses this definition to sum across all actions from state $s$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may also be useful to have this equation written instead only in terms of the action-value function. Right now, the RHS still incorporates the current value function. To remedy this, we will again use our definition of translating between value and action-value function by summing across all actions.\n",
    "\n",
    "$$ q_\\pi(a, s) = R_s^a + \\gamma \\sum_{s' \\in S} P_{ss'}^a \\sum_{a' \\in A} \\pi(a'|s') \\cdot q_\\pi(a', s') $$\n",
    "\n",
    "This is the same definition as above, except the future value portion is written as a sum of future action-value functions rather than simply the value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bellman Expectation Equation can be reduced to the Bellman Equation by selecting a policy $\\pi$. Remember, this simply redefines our problem now as an MDP. This means that it can once again be linearly solved:\n",
    "\n",
    "$$ v_\\pi = (I - \\gamma P^\\pi)^{-1} + R^\\pi $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important definition is the optimal state-value function - this is the maximum value function across all possible policies. Remember, a value function in an MRP is defined by the policy pertaining to it. If we look across all possible policies, we should find a value function that is the maximum for this given state. The same reasoning applies for the action-value function:\n",
    "\n",
    "$$ v_* (s) = \\max_{\\pi} v_\\pi (s) $$\n",
    "$$ q_* (a, s) = \\max_{\\pi} q_\\pi (s, a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider an MDP solved when we find the policy in the MRP that maximizes the value function, and therefore when we have found the optimal state-value function for each state. There is a theorem that states there is an optimal policy that is better than or equal to all other policies - that is, $v_{\\pi^*} \\geq v_{\\pi}$ for all pi. The theorem does need this policy to be unique, but any optimal policy must obtain the optimal value function for both the value function and the action-value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An optimal policy can be found by maximizing over the action-value functions. That is, given state $s$, our policy is to pick action $a$ if it maximizes the action-value function. That is \n",
    "\n",
    "$$\\pi_*(a | s) = 1 ~~\\mathrm{if}~~ a = \\mathrm{argmax}~ q_* (a, s)$$\n",
    "$$ = 0 ~ \\mathrm{otherwise} $$\n",
    "\n",
    "We only select action $a$ if it is the value maximizing action for that state $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this concept of an optimal policy, we can extend the **Bellman Expectation Equation** to the Bellman Optimality Equation. The following assert how the optimal value for any given state or state-action pair can be found by following the optimal policy for all future states.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_*(s) = \\max_a R_s^a + \\gamma \\sum_{s' \\in S} P_{ss'}^a \\cdot v_*(s') \\\\\n",
    "v_*(s) = \\max_a R_s^a + \\gamma \\sum_{s' \\in S} P_{ss'}^a \\cdot \\max_{a'} q_*(s', a') \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how this function no longer sums across all possible actions, but instead takes only the maximizing action at that state. This checks out with our earlier definition of the optimal policy being the one that takes the maximizing action for each state. Solving this equation is non-linear and requires iterative methods as described in future lectures/notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code/Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code block is used to define types used later \n",
    "\n",
    "from typing import TypeVar, Generic\n",
    "from typing import Mapping, Set, List, Tuple\n",
    "import numpy as np\n",
    "\n",
    "S = TypeVar('S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Markov Process consists of a set of states and state transition probabilities to dictate how one might move from one state to another. In constructing this class, I'd first like to make sure that states are represented generically. The only other data-type is a probability, which can surely be represented consitently as a float. It makes the most sense to me to represent this data-structure as a mapping from state1 to state2 to float, representing the probability of moving from state1 to state2. The set of states is thus easily maintained by the unique keys of the outer dictionary unioned with the unique keys of the inner dictionary. I've also provided a function and a member variable to represent the state probability transition matrix.\n",
    "\n",
    "NB: My type definitions here are very similar to the sample code from the course github. I took a look at the code first to better understand generic typing / type definitions in python, however attempted to limit looking at any functionality of the code in order to perform the exercise as independently as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MP(Generic[S]):\n",
    "    \n",
    "    # stp = state transition probability\n",
    "    def __init__(self, stp: Mapping[S, Mapping[S, float]] = None) -> None:\n",
    "        self.stp_ = stp\n",
    "        # a list to preserve order for transition matrix\n",
    "        self.states_ = stp.keys()\n",
    "        self.mat_ = self.tr_matrix()\n",
    "    \n",
    "    def get_transitions(self, state: S) -> Mapping[S, float]:\n",
    "        return self.stp_[state]\n",
    "    \n",
    "    def get_inbound(self, state: S) -> Set[S]:\n",
    "        \n",
    "        states = []\n",
    "        for in_state in self.stp_.keys():\n",
    "            if state in self.stp_[in_state].keys():\n",
    "                states.append(in_state)\n",
    "                \n",
    "        return states\n",
    "    \n",
    "    def update_transitions(self, state: S, transitions: Mapping[S, float]) -> None:\n",
    "        \n",
    "        total = 0\n",
    "        for v in transitions.values():\n",
    "            assert v >= 0, \"Probabilities must be non-negative\"\n",
    "            total += v\n",
    "        assert total == 1, \"Probabilities must sum to 1\"\n",
    "        self.stp_[state] = transitions\n",
    "        \n",
    "    def add_state(self, state: S, out: Mapping[S, float], inbound: List[Tuple[S, Mapping[S, float]]]) -> None:\n",
    "        \n",
    "        self.update_transitions(state, out)\n",
    "        for s, tr in inbound:\n",
    "            self.update_transitions(s, tr)\n",
    "        self.states_.add(state)\n",
    "        \n",
    "    def remove_state(self, state: S, inbound: List[Tuple[S, Mapping[S, float]]]) -> None:\n",
    "        \n",
    "        inbound_states = set(self.get_inbound(state))\n",
    "        arg_states = set()\n",
    "        for s, tr in inbound:\n",
    "            arg_states.add(s)\n",
    "        assert arg_states == inbound_states, \"Incorrect replacement set of inbound states\"\n",
    "        \n",
    "        for s, tr in inbound:\n",
    "            self.update_transitions(s, tr)\n",
    "            \n",
    "        self.states_.remove(state)\n",
    "        \n",
    "    def tr_matrix(self) -> np.ndarray:\n",
    "        \n",
    "        dim = len(self.states_)\n",
    "        mat = np.zeros((dim, dim))\n",
    "        for i, state1 in enumerate(self.states_):\n",
    "            for j, state2 in enumerate(self.states_):\n",
    "                try:\n",
    "                    mat[i][j] = self.stp_[state1][state2]\n",
    "                except:\n",
    "                    mat[i][j] = 0\n",
    "        return mat\n",
    "    \n",
    "    def sink_states(self) -> Set[s]:\n",
    "        states = set()\n",
    "        for k in self.stp_.keys():\n",
    "            if len(self.stp_[k]) == 1:\n",
    "                states.add(k)\n",
    "        return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {1: 0.1, 2: 0.6, 3: 0.1, 4: 0.2}, 2: {1: 0.25, 2: 0.22, 3: 0.24, 4: 0.29}, 3: {1: 0.7, 2: 0.3}, 4: {1: 0.3, 2: 0.5, 3: 0.2}, 5: {5: 1.0}}\n",
      "dict_keys([1, 2, 3, 4, 5])\n",
      "[5]\n"
     ]
    }
   ],
   "source": [
    "transitions = {\n",
    "        1: {1: 0.1, 2: 0.6, 3: 0.1, 4: 0.2},\n",
    "        2: {1: 0.25, 2: 0.22, 3: 0.24, 4: 0.29},\n",
    "        3: {1: 0.7, 2: 0.3},\n",
    "        4: {1: 0.3, 2: 0.5, 3: 0.2},\n",
    "        5: {5: 1.0}\n",
    "    }\n",
    "mp_obj = MP(transitions)\n",
    "print(mp_obj.stp_)\n",
    "print(mp_obj.states_)\n",
    "print(mp_obj.sink_states())\n",
    "# stationary = mp_obj.get_stationary_distribution()\n",
    "# print(stationary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Reward Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Markov Reward Process adds the concept of rewards to the prior Markov Process. The first goal of the code is to utilize the existing infrastructure for the MP. That is, it should be easy to translate from MP to MRP and vice versa. In order to do this, I'm going to use the same structure as before, however append a reward to each state in the dictionary. I am going to use the MP class to define most of the data, and then simply add another mapping to define the immediate rewards for each state. Note that this is the $R_s$ representation. I assume input data is stored as a mapping from state to a tuple of a mapping and a value (possible transitions and immediate reward).\n",
    "\n",
    "Note the functions for only obtaining the non-terminal states. This is because the reward, and all future reward, of arriving in a non-terminal state is 0. Thus, it is unnecessary to represent these states in the matrix representation that is used to solve for the value function. Even if a state transitions to a terminal state with non-zero probability, the value obtained from this possible transition is always zero and need not be included. Terminal states are represented as a set as order does not matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRP(MP):\n",
    "    \n",
    "    def __init__(self, data: Mapping[S, Tuple[Mapping[S, float], float]], gamma: float) -> None:\n",
    "        \n",
    "        # dictionaries to store data\n",
    "        stp = {}\n",
    "        rew = {}\n",
    "        for state in data.keys():\n",
    "            stp[state] = data[state][0]\n",
    "            rew[state] = data[state][1]\n",
    "        super().__init__(stp)\n",
    "        self.rewards_ = rew\n",
    "        self.gamma_ = gamma\n",
    "        # a list to preserve order for transition matrix\n",
    "        self.nt_states_ = self.get_nt_states()\n",
    "        # same order as nt states\n",
    "        self.rvec_ = self.rewards_vec()\n",
    " \n",
    "    def change_reward(self, state: S, r: float) -> None:\n",
    "        self.rewards_[state] = r\n",
    "        \n",
    "    def remove_state(self, state: S, inbound: List[Tuple[S, Mapping[S, float]]]) -> None:\n",
    "        super().remove_state(state, inbound)\n",
    "        self.rewards_.pop(state, None)\n",
    "        \n",
    "    def add_state(self, state: S, out: Mapping[S, float], inbound: List[Tuple[S, Mapping[S, float]]], r: float) -> None:\n",
    "        super().add_state(state, out, inbound)\n",
    "        self.rewards_[state] = r\n",
    "        \n",
    "    def rewards_vec(self) -> np.array:\n",
    "        r = [0] * len(self.nt_states_)\n",
    "        for i, state in enumerate(self.nt_states_):\n",
    "            r[i] = self.rewards_[state]\n",
    "        return np.asarray(r)\n",
    "    \n",
    "    def get_terminal_states(self) -> Set[S]:\n",
    "        term_states = super().sink_states()\n",
    "        return {s for s in term_states if self.rewards_[s] == 0}\n",
    "    \n",
    "    def get_nt_states(self) -> List[S]:\n",
    "        t_states = self.get_terminal_states()\n",
    "        return [s for s in self.states_ if s not in t_states]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6 0.3 0.1]\n",
      " [0.1 0.2 0.7]\n",
      " [0.  0.  1. ]]\n",
      "\n",
      "[ 7. 10.]\n",
      "\n",
      "{3}\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "        1: ({1: 0.6, 2: 0.3, 3: 0.1}, 7.0),\n",
    "        2: ({1: 0.1, 2: 0.2, 3: 0.7}, 10.0),\n",
    "        3: ({3: 1.0}, 0.0)\n",
    "    }\n",
    "mrp_obj = MRP(data, 1.0)\n",
    "print(mrp_obj.mat_)\n",
    "print()\n",
    "print(mrp_obj.rvec_)\n",
    "print()\n",
    "terminal = mrp_obj.get_terminal_states()\n",
    "print(terminal)\n",
    "#value_func_vec = mrp_obj.get_value_func_vec()\n",
    "#print(value_func_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
